# NetworkBot â€“ AIâ€‘powered network monitoring

## Overview

> **NetworkBot** is a web-based AI assistant for network monitoring. It uses a largeâ€‘language model (LLM) to answer questions about your infrastructure, with optional integration to UniFi Network and UniFi Site Manager.
>
> - Chat with the AI about status, logs, and diagnostics in a Matrix-style web UI.
> - Fetch current metrics from your monitoring system or paste log snippets for analysis.
> - Configure LLM (OpenAI or Ollama), monitoring integrations, and server settings from the same interface.
>
> NetworkBot is **Node.js** + **Express** and supports **OpenAI** or **Ollama** as the LLM provider.

## Features

| Feature | How it works |
|---------|--------------|
| **Web Chat** | Ask questions in the browser; the AI uses monitoring data when available and returns structured, human-readable answers. |
| **Web Configuration** | Configure LLM provider, API keys, models, UniFi Network/Site Manager, port, and authâ€”all from the UI. |
| **UniFi integrations** | Optional: pull devices and clients from UniFi Network; sites and devices from UniFi Site Manager (cloud). |
| **Structured AI output** | Status summary tables, error checks, conclusion, and next steps for easy scanning. |
| **Docker** | Docker Compose stack for deployment. |

## ðŸ“¦ Installation

1. **Clone the repo**:
   ```bash
   git clone https://github.com/your-username/networkbot-ai.git
   cd networkbot-ai
   ```
2. **Create a `.env` file** (optional; you can configure everything in the web UI):
   ```ini
   # LLM Provider (choose one)
   LLM_PROVIDER=openai  # or 'ollama'
   
   # For OpenAI
   OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   OPENAI_MODEL=gpt-4o-mini
   
   # For Ollama (if LLM_PROVIDER=ollama)
   # OLLAMA_BASE_URL=http://localhost:11434
   # OLLAMA_MODEL=llama2
   
   PORT=3000
   ```
3. **Install dependencies**:
   ```bash
   npm install
   ```
4. **Run**:
   ```bash
   npm start
   ```
   The app runs a **single server** on `http://localhost:3000` (or the port you set in config).
5. **Open the web interface**:
   - Go to `http://localhost:3000`
   - Log in (default: `admin` / `admin` â€” change this in Configuration!)
   - Use **Chat** to talk to the AI and **Config** to set LLM, monitoring, and server options.

## ðŸŒ Web Interface

### Accessing the Web Interface

1. Start NetworkBot: `npm start`
2. Open your browser to `http://localhost:3000`
3. Log in with your credentials (default: `admin` / `admin`)

### Features

- âœ… **Switch LLM Providers**: Toggle between OpenAI and Ollama with one click
- âœ… **Configure API Keys**: Securely store and update API keys
- âœ… **Model Selection**: Choose different models for each provider
- âœ… **Monitoring**: Configure UniFi Network and UniFi Site Manager from the web UIâ€”add/remove controllers, test connections
- âœ… **Test Connections**: Verify Ollama and each monitoring integration before saving
- âœ… **Server Settings**: Adjust port and log levels
- âœ… **Real-time Updates**: Changes take effect immediately (some may require restart)

### Security

- The web interface is protected with HTTP Basic Authentication
- Default credentials: `admin` / `admin` - **CHANGE THIS IN PRODUCTION!**
- Set custom credentials via environment variables:
  ```ini
  WEB_AUTH_USERNAME=your_username
  WEB_AUTH_PASSWORD=your_secure_password
  ```
- Or configure through the web interface itself (stored in `config.json`)

### Configuration Storage

- Settings are saved to `config.json` in the project root
- Environment variables take precedence over config file
- The web interface updates `config.json` directly

## ðŸ¤– Ollama Setup

NetworkBot supports **Ollama** for local LLM inference. This is perfect for privacy-sensitive environments or when you want to avoid API costs.

1. **Install Ollama**:
   - Visit [https://ollama.ai](https://ollama.ai) and download for your platform
   - Or use Docker: `docker run -d -p 11434:11434 ollama/ollama`

2. **Pull a model**:
   ```bash
   ollama pull llama2
   # or
   ollama pull mistral
   # or any other supported model
   ```

3. **Configure NetworkBot**:
   ```ini
   LLM_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=llama2
   ```

4. **Verify Ollama is running**:
   ```bash
   curl http://localhost:11434/api/tags
   ```

> **Note**: If running NetworkBot in Docker and Ollama on the host, use `OLLAMA_BASE_URL=http://host.docker.internal:11434` or configure Docker networking.

## ðŸ“¡ UniFi Network monitoring

NetworkBot can pull metrics from UniFi Network (devices, clients) using the **official UniFi Network API**.

- **API reference**: [UniFi Network API â€“ Getting started](https://developer.ui.com/network/v10.1.84/gettingstarted)
- **API key**: Create in **Network â†’ Control Plane â†’ Integrations** (or Controller **Settings â†’ API Access** on older versions).
- **Base URL**: Use your controller URL (e.g. `https://10.69.69.1` or `https://unifi.example.com`). For UDM Pro / UniFi OS, the app uses `/proxy/network` or `/unifi-api/network` automatically.
- Authentication uses the **X-API-Key** header when supported; local controllers fall back to session login with the same key.

Configure one or more UniFi Network controllers in the web UI under **Configuration â†’ Monitoring â†’ UniFi Network**.

## â˜ï¸ UniFi Site Manager (cloud)

NetworkBot can pull **sites and devices** from the cloud **UniFi Site Manager API** for a single UI account.

- **API reference**: [UniFi Site Manager API â€“ Getting started](https://developer.ui.com/site-manager/v1.0.0/gettingstarted)
- **Base URL**: `https://api.ui.com` (default)
- **Authentication**: **X-API-Key** header. Create a key in your UI account: **Settings â†’ API Keys** (EA) or **API** section (GA).
- **Rate limits**: EA 100 req/min; v1 stable 10,000 req/min (read-only).

Enable in the web UI under **Configuration â†’ Monitoring â†’ UniFi Site Manager (cloud)** (checkbox + API key + optional base URL), or in `config.json` â†’ `monitoring.siteManager`. Test with **Test Site Manager** or `POST /api/monitoring/test-site-manager`.

## ðŸš€ Usage

- Open the **Chat** tab and type a question (e.g. â€œSummarize UniFi device statusâ€ or â€œWhat do these logs indicate?â€).
- The AI uses monitoring data when configured and returns a structured reply (status summary, error check, conclusion, next steps).
- Use the **Configuration** tab to set the LLM provider, API keys, UniFi integrations, and server port/auth.

## ðŸ”§ Architecture

- **Express** serves the web UI and API (chat, config, monitoring tests).
- **LLM** queries go to OpenAI or Ollama based on configuration.
- **Monitoring** (optional) pulls data from UniFi Network and Site Manager and injects it into the AI context.

## ðŸ“ Project Structure

```
networkbot-ai/
â”œâ”€ app.js             # Express server, API routes, chat & config
â”œâ”€ public/            # Web UI (Matrix-style)
â”œâ”€ utils/
â”‚Â â”œâ”€ llm.js          # LLM query helper (OpenAI / Ollama)
â”‚Â â””â”€ logAnalyzer.js  # Config & monitoring (see config.js, monitoring.js)
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile
â”œâ”€ package.json
â””â”€ README.md
```

## Extending

| Goal | Where to extend |
|------|----------------|
| Query Prometheus | `utils/prometheus.js` â€“ add a function that runs ``curl`` or uses the official client and formats output for the LLM |
| Persist logs | Add a small SQLite database and `utils/logger.js` |
| Multiâ€‘language LLM | Replace the `OpenAIApi` call with a LangChain provider |

## Contributing

1. Fork the repo.
2. Create a feature branch (`git checkout -b feature/xxxx`).
3. Add tests under `__tests__`.
4. Ensure `npm test` passes.
5. Open a pull request.

## License

MIT â€“ feel free to adapt and distribute.

---

> **Tip** â€“ If you want a readyâ€‘loaded environment, use the Docker stack below.

## ðŸŽ‰ Docker Compose

The included `docker-compose.yml` provides:
- **NetworkBot service** with health checks
- **Optional Ollama service** (commented out by default)
- Proper networking configuration

### Build & run
```bash
docker compose up --build
```

The web interface will be available at `http://localhost:3000`.

### Running with Ollama in Docker
Uncomment the `ollama` service in `docker-compose.yml` and set `OLLAMA_BASE_URL=http://ollama:11434` in your `.env` file.

## âœ¨ Features & Improvements

- âœ… **Web Configuration Interface**: Beautiful UI for managing all settings
- âœ… **Dual LLM Support**: Works with both OpenAI and Ollama
- âœ… **Robust Error Handling**: Graceful error messages and logging
- âœ… **Matrix-style UI**: Dark theme with green/cyan terminal aesthetic
- âœ… **Enhanced Log Analysis**: Extracts errors, warnings, IPs, timestamps, and URLs
- âœ… **Environment Validation**: Checks required config on startup
- âœ… **Health Checks**: Docker health monitoring included
- âœ… **Better Logging**: Configurable log levels and structured output
- âœ… **Configuration Management**: JSON-based config with web UI and env var support
